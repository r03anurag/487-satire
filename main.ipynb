{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 487 Final Project: Stance Detection in Satire\n",
    "\n",
    "Anurag Renduchintala, Yoojin Bae, Karl Yan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to mount the Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount('/content/drive')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to import (and install) necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install portalocker\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following if using Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "'''# TODO: Change this to the path to your homework folder\n",
    "GOOGLE_DRIVE_PATH = '/content/drive/MyDrive/EECS 487/Homework_3'\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "os.chdir(GOOGLE_DRIVE_PATH)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab GPU Resources\n",
    "\n",
    "Check if GPU resources are available. If device = 'cpu', in the toolbar, click Runtime -> Change runtime type -> select GPU as the hardware accelerator.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "Google Colab imposes a **dynamic GPU usage limit** that depends on how much/long you use Colab. This is to keep Colab free for everyone. You can read about it [here](https://stackoverflow.com/questions/61126851/how-can-i-use-gpu-on-google-colab-after-exceeding-usage-limit). That being said, you should be able to complete this assignment without reaching your usage limit. You are **not** expected to spend your own money on Colab's paid GPU resources. In the event that you have run out of GPU resources, you would have to wait for resources or use a different Google account.\n",
    "\n",
    "Here are some tips to conserve your GPU usage:\n",
    "\n",
    "\n",
    "*   Change your runtime to GPU only when are working on parts that require GPU\n",
    "*   When spending long intervals on coding/taking a break, remember to disconnect your runtime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "!pip install readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to load the autoreload extension so that functions in python files will be re-imported into the notebook every time we run them. We also need to import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Task: Satire Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data by importing the necessary modules. Get the compiled final data frame. Tokenize all reviews (lowercasing will be done later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the raw datasets, putting together only the important columns (text and satire)\n",
    "from prepare_data import ALL_DATA\n",
    "# make a copy of this df. We don't want to modify the actual data.\n",
    "satire_data = ALL_DATA.copy()\n",
    "# lowercase all the headlines\n",
    "satire_data['text'] = satire_data['text'].str.lower()\n",
    "# tokenize all the text\n",
    "satire_data['tokenized_text'] = satire_data['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "# verify that our data is balanced\n",
    "print(len(satire_data))\n",
    "print(len(satire_data[satire_data[\"satire\"] == 0]))\n",
    "print(len(satire_data[satire_data[\"satire\"] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do Train, Test, Split; split the given data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#satire_data.to_csv(\"data_20231204.csv\") '''IGNORE THIS LINE'''\n",
    "#satire_data = pd.read_csv(\"data_20231204.csv\")  '''UNCOMMENT THIS LINE TO REPRODUCE RESULTS OBTAINED ON 12/4'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(satire_data[\"text\"], satire_data[\"satire\"], stratify=satire_data[\"satire\"])\n",
    "print(X_train.head(9))\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the BERT model preprocessor and encoder. Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "encoder_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess = hub.KerasLayer(preprocess_url)\n",
    "bert_encoder = hub.KerasLayer(encoder_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create BERT and Neural Network Layers, and then, create the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some hyperparameters first. Mess with these to see what you get.\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-2\n",
    "batch_size = 64\n",
    "reg = \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Layers\n",
    "input_ = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed = bert_preprocess(input_)\n",
    "output = bert_encoder(preprocessed)\n",
    "\n",
    "# NN Layers\n",
    "MODEL = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dropout(0.1, name='dropout', input_shape=(output['pooled_output'].shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', name='output', kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay) if reg==\"l2\" else tf.keras.regularizers.l1(l1=weight_decay))\n",
    "])\n",
    "\n",
    "# Build the final model\n",
    "model = tf.keras.Model(inputs=input_, outputs=MODEL(output['pooled_output']))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'), tf.keras.metrics.Precision(name=\"precision\"), \n",
    "                                                                     tf.keras.metrics.Recall(name=\"recall\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Evaluate, Make Predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model. Graph validation loss and accuracy by epoch.\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=batch_size, validation_split=.2)\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Make some predictions on new data (optional)\n",
    "# TODO: Write some code for this (later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will predict on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.flatten()\n",
    "# our y-pred values are sigmoid values between 0 and 1. So, we need to convert them.\n",
    "y_pred = [1 if score > 0.5 else 0 for score in y_pred]\n",
    "y_test_ = list(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization purposes, we calculate confusion matrix to assess our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate confusion matrix and accuracy\n",
    "cm = confusion_matrix(y_test_, y_pred)\n",
    "acc = accuracy_score(y_test_, y_pred)\n",
    "print(\"Accuracy: \", acc)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
